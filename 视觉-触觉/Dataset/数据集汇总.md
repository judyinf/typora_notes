# 数据集汇总

> 名称，采集设备，采集流程，规模与类型，数据组织结构，适用任务

[TOC]

### **Open X-Embodiment: Robotic Learning Datasets and RT-X Models**

https://robotics-transformer-x.github.io/



### **⭐Touch100k**: A Large-Scale Touch-Language-Vision Dataset for Touch-Centric Multimodal Representation





### ⭐TVL: A Touch, Vision, and Language Dataset for Multimodal Alignment 

https://tactile-vlm.github.io/

> **规模与构成：** 包含44,000对同步的“视觉-触觉”数据样本。数据采集自GelSight和DIGIT等光学触觉传感器。
>
> **标注瓶颈的突破：** 传统触觉数据缺乏自然语言标签（人类很难看一眼触觉压痕图就说出“这是摩擦系数0.5的橡胶”）。TVL创新性地引入了 **GPT-4V** 作为伪标注器。研究人员将同步采集的视觉图像输入GPT-4V，利用其强大的常识推理能力生成描述物体物理属性的文本（如“这是一个表面粗糙且编织紧密的织物，按压时有轻微弹性”）。
>
> **三模态对齐：** 通过这些伪标签，数据集建立了 {触觉图像 $\leftrightarrow$ 视觉图像 $\leftrightarrow$ 自然语言描述} 的三角映射关系。这使得模型不仅能学习“视觉-触觉”的物理关联，还能理解“抓紧一点”或“轻抚”等语言指令对应的触觉特征。
>
> 开发了基于这些编码器的生成模型，以通过视觉和触觉输入生成触觉的语言描述。

- a vision-language aligned tactile encoder 视觉-语言对齐的触觉编码器

- a tactile-vision-language model (TVLM) for describing tactile sensations 生成语言描述

- 区分表面纹理、物体材料、尺寸和接触力

  



###  ⭐Visuo-Tactile Video (VTV) : Universal Visuo-Tactile Video Understanding for Embodied Interaction

https://arxiv.org/html/2505.22566v1

> - **规模：** 150,000帧视频数据，覆盖100种日常物体。
> - **结构化属性标注：** 不同于自由文本，VTV对四大核心物理属性进行了结构化分级标注：
>   - **硬度 (Hardness):** 高度可变形 (28%) / 中度可变形 (33%) / 极硬 (39%)。
>   - **突起度 (Protrusion):** 无 (41%) / 中度 (26%) / 强 (33%)。
>   - **弹性 (Elasticity):** 无 (42%) / 中度 (30%) / 强 (28%)。
>   - **摩擦力 (Friction):** 轻微 (32%) / 中度 (25%) / 强 (43%)。



###  ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image

https://arxiv.org/abs/2505.20498

> 合成数据







### DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation 

https://pku-mocca.github.io/Dextercap-Page/



### MOMAGEN: GENERATING DEMONSTRATIONS UNDER SOFT AND HARD CONSTRAINTS FOR MULTI-STEP BIMANUAL MOBILE MANIPULATION 

https://arxiv.org/pdf/2510.18316



### ⭐Touch and Go: Learning from Human-Collected Vision and Touch

https://arxiv.org/pdf/2211.12498



### ⭐ObjectFolder

https://objectfolder.stanford.edu/



### ⭐SSVTP: Self-Supervised Visuo-Tactile Pretraining to Locate and Follow Garment Features 

https://www.roboticsproceedings.org/rss19/p018.pdf



### Towards Comprehensive Multimodal Perception: Introducing the Touch-Language-Vision Dataset

https://arxiv.org/abs/2403.09813v1
