# 数据集汇总

> 名称，采集设备，采集流程，规模与类型，数据组织结构，适用任务

[TOC]

### **Open X-Embodiment: Robotic Learning Datasets and RT-X Models**

https://robotics-transformer-x.github.io/

**描述**: 目前最大的开源机器人操作数据集，由 Google DeepMind 联合全球 30+ 实验室发布。

**规模**: 包含 200 多万条轨迹，涵盖 22 种机器人本体。

**数据类型**: 极度**异构 (Heterogeneous)**。

- 有的包含深度图，有的只有 RGB。
- 有的动作是笛卡尔空间 (End-effector)，有的是关节空间 (Joint)。
- 有的是遥操采集，有的是脚本生成。

**采集流程**: 各个实验室独立采集。例如，有的实验室使用 VR 手柄遥操 Franka 机器人做厨房任务，有的使用脚本控制 WidowX 机器人抓取物体。OXE 将这些数据标准化为统一的 RLDS (Reinforcement Learning Datasets) 格式。

### **⭐Touch100k**: A Large-Scale Touch-Language-Vision Dataset for Touch-Centric Multimodal Representation





### ⭐TVL: A Touch, Vision, and Language Dataset for Multimodal Alignment 

https://tactile-vlm.github.io/

> **规模与构成：** 包含44,000对同步的“视觉-触觉”数据样本。数据采集自GelSight和DIGIT等光学触觉传感器。
>
> **标注瓶颈的突破：** 传统触觉数据缺乏自然语言标签（人类很难看一眼触觉压痕图就说出“这是摩擦系数0.5的橡胶”）。TVL创新性地引入了 **GPT-4V** 作为伪标注器。研究人员将同步采集的视觉图像输入GPT-4V，利用其强大的常识推理能力生成描述物体物理属性的文本（如“这是一个表面粗糙且编织紧密的织物，按压时有轻微弹性”）。
>
> **三模态对齐：** 通过这些伪标签，数据集建立了 {触觉图像 $\leftrightarrow$ 视觉图像 $\leftrightarrow$ 自然语言描述} 的三角映射关系。这使得模型不仅能学习“视觉-触觉”的物理关联，还能理解“抓紧一点”或“轻抚”等语言指令对应的触觉特征。
>
> 开发了基于这些编码器的生成模型，以通过视觉和触觉输入生成触觉的语言描述。

- a vision-language aligned tactile encoder 视觉-语言对齐的触觉编码器

- a tactile-vision-language model (TVLM) for describing tactile sensations 生成语言描述

- 区分表面纹理、物体材料、尺寸和接触力

  



###  ⭐Visuo-Tactile Video (VTV) : Universal Visuo-Tactile Video Understanding for Embodied Interaction

https://arxiv.org/html/2505.22566v1

> - **规模：** 150,000帧视频数据，覆盖100种日常物体。
> - **结构化属性标注：** 不同于自由文本，VTV对四大核心物理属性进行了结构化分级标注：
>   - **硬度 (Hardness):** 高度可变形 (28%) / 中度可变形 (33%) / 极硬 (39%)。
>   - **突起度 (Protrusion):** 无 (41%) / 中度 (26%) / 强 (33%)。
>   - **弹性 (Elasticity):** 无 (42%) / 中度 (30%) / 强 (28%)。
>   - **摩擦力 (Friction):** 轻微 (32%) / 中度 (25%) / 强 (43%)。



###  ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image

https://arxiv.org/abs/2505.20498

> 合成数据







### DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation 

https://pku-mocca.github.io/Dextercap-Page/



### MOMAGEN: GENERATING DEMONSTRATIONS UNDER SOFT AND HARD CONSTRAINTS FOR MULTI-STEP BIMANUAL MOBILE MANIPULATION 

https://arxiv.org/pdf/2510.18316



### ⭐Touch and Go: Learning from Human-Collected Vision and Touch

https://arxiv.org/pdf/2211.12498

**采集设备 (Sensor):** GelSight (光学触觉传感器)

**规模 (Scale):** 120k (12万个数据对)

**采集主体:** **人类 (Human)**

**采集流程:**

- **过程:** 这是一个“野外”（In-the-wild）数据集。采集者佩戴设备在各种真实环境（如街道、办公室、家庭）中行走，并用手持的触觉传感器接触各种物体表面。
- **操作类型:** 探查性触摸 (Touching/Probing)，主要是按压和接触材质表面。

**视觉视角:**

- **视角:** **第一人称/以自我为中心 (Egocentric)**。通常通过佩戴在采集者身上的相机或与触觉传感器绑定的相机拍摄。

**数据组织结构:** 包含配对的视觉图像（宏观物体图）和触觉图像（微观纹理图）。

**适用任务:** 材质分类 (Material Classification)、视触觉跨模态生成、自我监督学习。



### ⭐ObjectFolder

https://objectfolder.stanford.edu/



### ⭐SSVTP: Self-Supervised Visuo-Tactile Pretraining to Locate and Follow Garment Features 

https://www.roboticsproceedings.org/rss19/p018.pdf



### Towards Comprehensive Multimodal Perception: Introducing the Touch-Language-Vision Dataset

https://arxiv.org/abs/2403.09813v1



### ObjTac

> OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing



### DROID 





### **The Feeling of Success**

**采集设备 (Sensor):** GelSight

**规模 (Scale):** 9.3k (9300次抓取尝试)

**采集主体:** **机器人 (Robot)**

**采集流程:**

- **过程:** 这是一个经典的机器人抓取数据集。机器人尝试抓取各种物体，记录抓取过程中的视觉和触觉反馈，以及最终是否成功抓起的标签。
- **操作类型:** **抓取 (Grasping)**。

**视觉视角:**

- **视角:** 通常包含 **第三人称固定视角** (拍摄整个工作台) 和 **眼在手上 (Eye-in-hand)** 视角。

**数据组织结构:** 视觉图像 + 抓取前/抓取中的触觉图像 + 抓取结果标签（成功/失败）。

**适用任务:** 机器人抓取 (Robot Grasp)、抓取稳定性预测、滑移检测。



### YCB-Slide

**采集设备 (Sensor):** DIGIT (低成本光学触觉传感器)

**规模 (Scale):** 183k (18.3万帧)

**采集主体:** **机器人 (Robot)**

**采集流程:**

- **过程:** 控制机械臂携带 DIGIT 传感器，在 YCB 标准物体集（一组标准化的日常物体）表面进行滑动操作。
- **操作类型:** **滑动 (Sliding)**。这种操作为了获取连续的纹理变化。

**视觉视角:**

- **视角:** 主要是记录物体的外观，通常配合外部固定相机。

**数据组织结构:** 连续的时间序列数据，包含触觉流和对应的物体类别/材质标签。

**适用任务:** 材质分类 (Material Classification)、纹理识别。







| **数据集名称**       | **类型**        | **采集方式**                     | **触觉传感器类型**                               | **特点**                                                     |
| -------------------- | --------------- | -------------------------------- | ------------------------------------------------ | ------------------------------------------------------------ |
| **ObjectFolder 2.0** | **仿真 (Sim)**  | 虚拟生成                         | **Simulated GelSight** (接触面深度图/RGB)        | **多模态对象库**。包含 1000 个物体的隐式神经表示，可同时渲染视觉、听觉和触觉（GelSight 风格）数据。适合 Sim-to-Real 预训练。 |
| **FreeTacMan**       | **真机 (Real)** | **人类手持采集** (Human-centric) | **Visuo-Tactile** (类 GelSight 的视觉触觉传感器) | **大规模操纵数据集**。使用手持夹爪采集，包含 300 万对视触觉图像和 10k 条轨迹。涵盖 50 种富接触任务（如插拔、按压、摸索）。 |
| **Touch and Go**     | **真机 (Real)** | **人类“野外”采集** (In-the-wild) | **GelSight / DIGIT**                             | **自然场景探针**。人类佩戴摄像机和触觉传感器在日常环境中“戳”各种物体。包含丰富的材质纹理数据，适合学习视触觉对齐（Alignment）。 |
| **VTDexManip**       | **真机 (Real)** | 人类操作/灵巧手                  | **稀疏/二值触觉** (Contact/Force)                | **灵巧手操纵**。包含 10 种日常任务和 182 个物体。虽然触觉信息较稀疏（主要是接触状态和力），但专注于复杂的灵巧操作技能学习。 |
| **SSVTP** / **TVL**  | **真机 (Real)** | 机器人采集                       | **DIGIT** (低成本 GelSight)                      | **视-触-语言对齐**。结合了机器人采集（SSVTP）和人类采集数据，专门用于训练 Tactile-VLM，包含语言描述标签。 |
