# 视觉-触觉调研

[TOC]

## 引言

> 为什么要引入触觉？

- 视觉不仅容易受到**光照变化、镜面反射和物体纹理缺失**的干扰，更致命的是在接触操作（Contact-Rich Manipulation）中不可避免的遮挡问题：当机械手接近并抓取物体时，手掌和手指往往会完全遮挡相机的视线，导致感知盲区的出现。
- ✔️ 抓握问题
- 触觉（Tactile Sensing）是唯一能够直接感知**接触界面物理属性**（如力、压力分布、摩擦系数、温度、硬度及微观纹理）的模态。
- 尽管视觉数据（RGB-D）在捕捉几何结构和空间关系方面表现出色，但它在推断质量、摩擦力、柔顺度（Compliance）和纹理等物理属性时存在天然的模糊性与遮挡问题 。
- 触觉的重要性在生物学上已被反复证实——人类可以闭眼完成复杂的灵巧操作

> 为什么在机器人领域，触觉的引入长期滞后？

- 这主要归因于触觉传感器的**硬件异构性**（从压阻阵列到视触觉GelSight）、**数据采集的高昂成本**（缺乏类似ImageNet的触觉数据集）以及**缺乏统一的特征表征方法**。

> 大模型时代的研究趋势

- 将自监督学习（Self-Supervised Learning, SSL）、Transformer架构以及生成式模型（Diffusion Models）应用于触觉数据
- 仿真技术的突破以及Sim-to-Real（仿真到现实）的迁移策略

> ⭐描述接触界面物理属性的物理量有哪些？

弹性体硬度，刚度，粗糙度，摩擦系数，压力分布，微观纹理

声音，温度



### 学习范式

**“多模态训练 + 单模态推理”**

- 特权信息学习（Learning Using Privileged Information, LUPI）

- 跨模态感知

- 跨模态蒸馏（Cross-Modal Distillation）：将触觉感知的物理直觉“蒸馏”进纯视觉策略中

- 硬件上只保留最鲁棒的RGB-D相机和本体传感器，而细腻的触觉交互能力则通过云端下载的、在海量特权数据上预训练好的“触觉大模型”来赋予




## 理论基础

### 1. 特权信息学习（LUPI）的数学表述

LUPI范式引入了一个“教师策略”（Teacher Policy），其观察空间包含了特权信息 $O_{priv} = \{I_{rgb}, T_{tactile}, P_{proprio}, L_{text}\}$。触觉信息 $T_{tactile}$ 直接消除了物理交互中的不确定性（例如，直接测量到滑移而非通过视觉光流推断）。训练目标不仅是最大化任务奖励，更是最小化“学生策略”（Student Policy，仅输入 $O_{vis}$）与教师策略之间的分布差异：
$$
\mathcal{L}_{distill} = D_{KL}(\pi_{teacher}(\cdot | O_{priv}) |

| \pi_{student}(\cdot | O_{vis})) 
$$


通过这种方式，学生网络的视觉编码器被迫去关注那些与触觉特征高度相关的视觉线索（例如，手指接触物体时指腹的微小形变，或线缆在受力时的张力曲线），从而在潜空间（Latent Space）中**重建出触觉信息的“代理特征”** 。

### 2. 机器人联觉（Robotic Synesthesia）

潜空间的语义对齐

当视觉编码器看到“丝绸”时，它生成的向量与触觉编码器触摸“丝绸”生成的向量高度相似。在推理阶段，即使没有触觉输入，视觉编码器生成的向量已经包含了“光滑、低摩擦”的物理属性含义，下游的策略网络（Policy Network）便能据此生成相应的轻柔抓取动作。

> "A Touching Sight: SII/PV Activation during the Observation and Experience of Touch"
>
> https://www.sciencedirect.com/science/article/pii/S0896627304001564#aep-section-id47

### 3. 动态的生成式预测



## 挑战

### 推理阶段

- **微观纹理失效：** 视觉编码器受限于**相机分辨率**。如果物体表面的摩擦力差异是由微米级的纹理决定的（例如肉眼看不出的油渍），视觉模型将无法生成正确的触觉预测，导致灾难性的滑脱。
- **未见物体的泛化：** 对于从未见过的合成材料（例如外观像金属但触感像海绵的物体），基于视觉先验的预测会产生严重的**确认偏差（Confirmation Bias）**，导致机器人施加错误的抓取力。当前的生成模型在处理“视错觉”时仍显脆弱。例如，视觉上看起来粗糙的物体（如印有石头纹理的光滑塑料），会被模型错误地生成为“粗糙”的触觉特征。Octopi 等系统目前严重依赖视觉先验。残差学习（Residual Learning）。
- 引入生成式世界模型（尤其是基于Diffusion的）会显著**增加推理延迟**。

### 数据收集

- 在机器人操作过程中收集（时间序列）和学习数据的自监督方法  e.g. Hand-Object Manipulation
- 触觉传感器面临着严峻的工程挑战：硬件成本高昂、弹性体表面易磨损、布线复杂以及数据传输带宽受限等问题，使得在每一台服务机器人上标配高分辨率触觉传感器变得不切实际 。

**数据稀缺性 (Data Scarcity)**：

- Open X-Embodiment 等大数据集主要包含图像和动作，极少包含触觉数据。
- **触觉数据无法从互联网获取**。必须通过机器人实地接触采集。目前最大的触觉数据集（如 Touch-and-Go, SSVTP）也仅有几万到几十万的样本。

**遥操作中的“感觉丧失”**：

- 采集高质量演示数据通常依赖遥操作（Teleoperation）。但目前的操作设备（如 VR 手柄）**缺乏力反馈**。
- 人类操作员在采集数据时“感觉不到”机器人摸到了什么，导致采集到的动作数据往往是基于视觉判断的，并没有包含真正的“触觉交互逻辑”（例如：通过手感微调力度的动作）。这使得模仿学习（BC）很难学到触觉的因果关系。

**对策**：需要依赖 Sim-to-Real（如在 Isaac Gym 中模拟触觉）或使用小样本微调（Few-shot Finetuning）。

### 仿真层面的挑战

>  Sim-to-Real 的鸿沟 (The Sim-to-Real Chasm)

视觉仿真已经很成熟（光线追踪），但触觉仿真在物理和渲染上都极难。

- **接触动力学难模拟 (Complex Contact Dynamics)**：
  - 刚体模拟（Rigid Body Physics）很快但不准。
  - **软体/形变模拟 (Soft Body/Deformation)**：真实的触觉传感器（如 GelSight 的硅胶层，或手指的皮肤）是柔性的。模拟这种非线性形变、各向异性摩擦、微小滑移（Micro-slip）需要 **FEM (有限元分析)**，计算量巨大，难以用于大规模 RL 训练（RL 需要数百万步）。
- **视触觉渲染难 (Optical Tactile Rendering)**：
  - 对于 GelSight 这类基于光学的传感器，不仅要算力，还要算出光线在硅胶内部的折射、散射。虽然有 **Taxim** 或 **TacSL** 这样的工具，但在光照一致性和纹理细节上，仿真与现实仍有巨大差距（Domain Gap）。
- **对策**：利用 AI 神经渲染（NeRF/Gaussian Splatting）来提升触觉仿真逼真度。

### 模态对齐

**采样频率不匹配 (Temporal Mismatch)**：

- 视觉和触觉以**不同的频率**提供反馈，而本体感觉提供连续的反馈。视觉通常 30-60 Hz，触觉传感器的采样率通常在 **1000 Hz (1kHz)** 以上。需要能够捕获**跨模态**的**时间相关性**的架构上，特别是为了理解**动态对象属性**，例如变形性和弹性。


**时空分辨率严重不匹配 (Spatiotemporal Mismatch)**：

- **时间上**：视觉是低频的（30Hz），触觉是高频的（500Hz-2kHz）。如果在 Transformer 中简单对齐，高频触觉会被低频视觉“拖累”，丢失震动纹理信息；如果不降频，Token 数量会爆炸。
- **空间上**：视觉是全局的（Global），触觉是极其局部的（Local）。如何让模型理解“指尖这一点的力”对应“图像中这一个像素的物体”，需要极强的空间注意力机制。

**对策**：在 Tokenization 阶段，对触觉数据进行**时间维度的压缩**（如将 100ms 内的触觉信号编码为一个 Feature Vector），或者设计多速率 Transformer。

**Cross-Embodiment 统一表征**：像 3D-ViTac 那样，用 3D 点云作为统一接口，抹平不同传感器的差异。

**异构性平衡**：

- RDT 容易忽略触觉，因为大部分时间（自由空间运动）触觉都是噪音或零值。
- **模态坍塌 (Modality Collapse / Modality Laziness)**：
  - 在端到端训练中，神经网络倾向于走捷径。如果视觉本身能解决 90% 的问题（比如抓取一个大物体），网络会倾向于**忽略触觉通道**，把触觉权重降为零。
- **挑战**：如何设计训练目标（如 Masked Modality Modeling），强迫模型在视觉受限（遮挡）时必须依赖触觉，而在自由空间运动时忽略触觉噪声？
- **对策**：使用 **Masked Training**。在训练时随机 Mask 掉视觉输入，强迫模型学会“盲操作”，从而学会依赖触觉。类似 MAE (Masked Autoencoders) 的自监督学习，让模型先学会理解物理接触，再做任务。

### 泛化性

**非标准化 (Lack of Standardization)**：

- 相机只有 RGB，格式统一。触觉传感器千奇百怪（压阻阵列、光学图像、磁性霍尔效应、单点力矩）。
- 这导致很难像 Open X-Embodiment 那样跨机器人联合训练。在一个传感器上训练的模型，几乎无法迁移到另一种传感器上。



### 方向

- **事件驱动的 Tokenization (Event-based Tokenization)**: 不再按固定频率采样触觉，而是仅在力发生突变（接触/滑移）时生成 Token，解决高频丢失问题。

- **动态权重门控 (Dynamic Gating)**: 设计专门的 Gating Network，根据不确定性（Uncertainty）实时决定相信视觉还是触觉，而不是依赖隐式的 Attention。

- **对比学习预对齐 (Contrastive Pre-alignment)**: 在输入 VLA 之前，先通过对比学习强行对齐视触觉特征空间，减少融合阶段的干扰。

-  快慢控制层级（Slow-Fast Hierarchy）
  - **慢回路（Slow Loop）**：基于视觉和扩散模型，生成全局的、长时程的动作规划（Waypoints），频率较低（如几赫兹）。

  - **快回路（Fast Loop）**：基于实时触觉反馈，通过一个轻量级的残差网络或导纳控制器，对慢回路生成的轨迹进行高频修正。


- 根据前后时序的因果性等角度

- 数据质量与多样性比单纯数据量更关键，不同数据混合策略会带来不同模型特性。



## 其它资料